{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f9bce-0075-4bee-b2b5-434611f104b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Jobenn Bezuidenhout u22518500\n",
    "# TRAIN AND FINE-TUNE LLMs on AMD GPU (DirectML) - Same outputs\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_directml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "import evaluate  # kept for parity with your original (though we compute accuracy directly too)\n",
    "from tqdm.auto import tqdm\n",
    "# -----------------------------\n",
    "# Device: AMD GPU via DirectML\n",
    "# -----------------------------\n",
    "dml = torch_directml.device()\n",
    "print(\"Using device:\", dml)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load the combined preprocessed CSV from Phase 2\n",
    "# ---------------------------------------------\n",
    "combined_df = pd.read_csv('Jobenn_preprocessed_NCF_data.csv')\n",
    "\n",
    "separator = ' [SEP] '\n",
    "\n",
    "split_parts = combined_df['text'].apply(\n",
    "    lambda x: x.split(separator, 1) if separator in x else [x, '']\n",
    ")\n",
    "split_df = pd.DataFrame(split_parts.tolist(), index=combined_df.index, columns=['pm_text', 'ugr_text'])\n",
    "combined_df = pd.concat([combined_df, split_df], axis=1)\n",
    "combined_df = combined_df.drop(columns=['text'])\n",
    "\n",
    "# Map string labels to integers in the DataFrame (0 for 'Benign', 1 for 'Ransomware')\n",
    "combined_df['label'] = [0 if str(l).lower() == 'benign' else 1 for l in combined_df['label']]\n",
    "\n",
    "# Split: 80/20\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['label'], random_state=42)\n",
    "\n",
    "# Handle class imbalance: Compute weights\n",
    "classes = np.unique(train_df['label'])\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_df['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# --------------------------------------\n",
    "# 1) FAST Preprocessing Function (same)\n",
    "# --------------------------------------\n",
    "def preprocess_fast(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples['pm_text'],\n",
    "        examples['ugr_text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "columns_to_remove = ['protocol', 'flag', 'family', 'address', 'usd', 'seed_address',\n",
    "                     'btc', 'netflow_bytes', 'ip_address', 'clusters', 'threats',\n",
    "                     'port', 'time', 'prediction', 'segment', 'embeddings',\n",
    "                     'dataset', 'r', 'rw', 'rx', 'rwc', 'rwx', 'rwxc', 'category',\n",
    "                     'pm_text', 'ugr_text']\n",
    "\n",
    "# --------------------------\n",
    "# Tokenize (BERT + RoBERTa)\n",
    "# --------------------------\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_ds_bert = train_dataset.map(\n",
    "    lambda e: preprocess_fast(e, bert_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in columns_to_remove if c in train_dataset.column_names]\n",
    ")\n",
    "test_ds_bert = test_dataset.map(\n",
    "    lambda e: preprocess_fast(e, bert_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in columns_to_remove if c in test_dataset.column_names]\n",
    ")\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "train_ds_roberta = train_dataset.map(\n",
    "    lambda e: preprocess_fast(e, roberta_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in columns_to_remove if c in train_dataset.column_names]\n",
    ")\n",
    "test_ds_roberta = test_dataset.map(\n",
    "    lambda e: preprocess_fast(e, roberta_tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[c for c in columns_to_remove if c in test_dataset.column_names]\n",
    ")\n",
    "\n",
    "# HF Datasets expect label->labels for torch format\n",
    "def prepare_for_torch(ds, tokenizer):\n",
    "    ds = ds.rename_column('label', 'labels')\n",
    "    cols = ['input_ids', 'attention_mask', 'labels']\n",
    "    # Include token_type_ids if the tokenizer provides them (e.g., BERT)\n",
    "    if 'token_type_ids' in tokenizer.model_input_names:\n",
    "        cols.append('token_type_ids')\n",
    "    ds.set_format(type='torch', columns=cols)\n",
    "    return ds, ('token_type_ids' in tokenizer.model_input_names)\n",
    "\n",
    "train_ds_bert_torch, bert_uses_token_type = prepare_for_torch(train_ds_bert, bert_tokenizer)\n",
    "test_ds_bert_torch, _ = prepare_for_torch(test_ds_bert, bert_tokenizer)\n",
    "\n",
    "train_ds_roberta_torch, roberta_uses_token_type = prepare_for_torch(train_ds_roberta, roberta_tokenizer)\n",
    "test_ds_roberta_torch, _ = prepare_for_torch(test_ds_roberta, roberta_tokenizer)\n",
    "\n",
    "# DataLoaders — we simulate effective batch_size=16 with grad accumulation\n",
    "MICRO_BATCH = 4      # micro-batch per step (fits VRAM)\n",
    "ACCUM_STEPS = 4      # 4 * 4 = 16 \"per_device_train_batch_size\" effective\n",
    "BATCH_EVAL  = 16     # eval batch\n",
    "\n",
    "train_loader_bert = DataLoader(train_ds_bert_torch, batch_size=MICRO_BATCH, shuffle=True, num_workers=0)\n",
    "test_loader_bert  = DataLoader(test_ds_bert_torch,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=0)\n",
    "\n",
    "train_loader_roberta = DataLoader(train_ds_roberta_torch, batch_size=MICRO_BATCH, shuffle=True, num_workers=0)\n",
    "test_loader_roberta  = DataLoader(test_ds_roberta_torch,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=0)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) \"Trainer-like\" wrapper so .state.log_history still exists\n",
    "# -------------------------------------------------------------\n",
    "class _State:\n",
    "    def __init__(self):\n",
    "        self.log_history = []  # will hold dicts with 'loss' and 'eval_loss' like HF\n",
    "\n",
    "class TrainerLike:\n",
    "    def __init__(self):\n",
    "        self.state = _State()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Fine-tune function (DirectML, class weights)\n",
    "#    - Mirrors your metrics/plots/attention output\n",
    "# -------------------------------------------------\n",
    "def fine_tune_model_directml(model_name, train_loader, test_loader, class_weights,\n",
    "                             uses_token_type_ids: bool, tokenizer):\n",
    "    trainer_like = TrainerLike()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, dtype=torch.float32 )\n",
    "    \n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    model.to(dml)\n",
    "    model.train()\n",
    "\n",
    "    num_epochs = 3\n",
    "    lr = 5e-5\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    total_steps = num_epochs * math.ceil(len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=max(1, int(0.06 * total_steps)),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(dml))\n",
    "\n",
    "    global_step = 0\n",
    "    train_losses_per_step = []\n",
    "    eval_losses_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ---- TRAIN ----\n",
    "        train_bar = tqdm(train_loader, \n",
    "                        desc=f'Epoch {epoch+1}/{num_epochs} [Train]', \n",
    "                        leave=True)\n",
    "        \n",
    "        for step, batch in enumerate(train_bar, start=1):\n",
    "            # Build inputs dict dynamically (skip token_type_ids if not present)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(dml),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(dml),\n",
    "            }\n",
    "            if uses_token_type_ids and \"token_type_ids\" in batch:\n",
    "                inputs[\"token_type_ids\"] = batch[\"token_type_ids\"].to(dml)\n",
    "            labels = batch[\"labels\"].to(dml)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            if step % ACCUM_STEPS == 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                # Clear GPU cache periodically\n",
    "                if hasattr(torch, 'dml'):\n",
    "                    torch.dml.empty_cache()\n",
    "\n",
    "            running += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'step': f'{step}/{len(train_loader)}'\n",
    "            })\n",
    "\n",
    "            # record a training loss log like HF (per step)\n",
    "            trainer_like.state.log_history.append({\n",
    "                'loss': float(loss.item()),\n",
    "                'step': int(global_step),\n",
    "                'epoch': float(epoch + (step / len(train_loader)))\n",
    "            })\n",
    "            train_losses_per_step.append(float(loss.item()))\n",
    "\n",
    "        train_bar.close()\n",
    "\n",
    "        # ---- EVAL ----\n",
    "        model.eval()\n",
    "        val_running = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        eval_bar = tqdm(test_loader, \n",
    "                       desc=f'Epoch {epoch+1}/{num_epochs} [Eval]', \n",
    "                       leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_bar:\n",
    "                inputs = {\n",
    "                    \"input_ids\": batch[\"input_ids\"].to(dml),\n",
    "                    \"attention_mask\": batch[\"attention_mask\"].to(dml),\n",
    "                }\n",
    "                if uses_token_type_ids and \"token_type_ids\" in batch:\n",
    "                    inputs[\"token_type_ids\"] = batch[\"token_type_ids\"].to(dml)\n",
    "                labels = batch[\"labels\"].to(dml)\n",
    "\n",
    "                out = model(**inputs)\n",
    "                loss = criterion(out.logits, labels)\n",
    "                val_running += loss.item()\n",
    "\n",
    "                preds = out.logits.argmax(dim=1)\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                # Update eval progress bar\n",
    "                eval_bar.set_postfix({\n",
    "                    'eval_loss': f'{loss.item():.4f}'\n",
    "                })\n",
    "        eval_bar.close()\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        eval_loss = val_running / max(1, len(test_loader))\n",
    "        eval_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        eval_losses_per_epoch.append(eval_loss)\n",
    "        trainer_like.state.log_history.append({\n",
    "            'eval_loss': float(eval_loss),\n",
    "            'eval_accuracy': float(eval_acc),\n",
    "            'epoch': float(epoch + 1)\n",
    "        })\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/3  train_loss(avg/step): {running/len(train_loader):.4f} | eval_loss: {eval_loss:.4f} | eval_acc: {eval_acc:.4f}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    # ---- Plot loss (same style as your code expects) ----\n",
    "    train_loss = [log['loss'] for log in trainer_like.state.log_history if 'loss' in log]\n",
    "    eval_loss  = [log['eval_loss'] for log in trainer_like.state.log_history if 'eval_loss' in log]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(range(1, len(eval_loss)+1), eval_loss, label='Eval Loss')\n",
    "    plt.xlabel('Steps/Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curve for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Attention heatmap on a sample (like your original) ----\n",
    "    with torch.no_grad():\n",
    "        sample_text_1 = test_df['pm_text'].iloc[0]\n",
    "        sample_text_2 = test_df['ugr_text'].iloc[0]\n",
    "\n",
    "        sample_input = tokenizer(\n",
    "            sample_text_1,\n",
    "            sample_text_2,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128\n",
    "        )\n",
    "        sample_input = {k: v.to(dml) for k, v in sample_input.items()}\n",
    "\n",
    "        outputs = model(**sample_input, output_attentions=True)\n",
    "        attn = outputs.attentions[-1][0][0].detach().cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(attn, cmap='viridis')\n",
    "        plt.title(f'Attention Weights for {model_name} (Sample)')\n",
    "        plt.show()\n",
    "\n",
    "    return trainer_like, model, tokenizer\n",
    "\n",
    "# -------------------------------------\n",
    "# 4) Train BERT and RoBERTa (as before)\n",
    "# -------------------------------------\n",
    "bert_trainer, bert_model, bert_tokenizer = fine_tune_model_directml(\n",
    "    'bert-base-uncased',\n",
    "    train_loader_bert,\n",
    "    test_loader_bert,\n",
    "    class_weights,\n",
    "    uses_token_type_ids=bert_uses_token_type,\n",
    "    tokenizer=bert_tokenizer\n",
    ")\n",
    "\n",
    "roberta_trainer, roberta_model, roberta_tokenizer = fine_tune_model_directml(\n",
    "    'roberta-base',\n",
    "    train_loader_roberta,\n",
    "    test_loader_roberta,\n",
    "    class_weights,\n",
    "    uses_token_type_ids=roberta_uses_token_type,\n",
    "    tokenizer=roberta_tokenizer\n",
    ")\n",
    "\n",
    "# (Optional) DeBERTa — uncomment if needed and VRAM allows\n",
    "# de_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "# train_ds_deberta = train_dataset.map(lambda e: preprocess_fast(e, de_tokenizer), batched=True,\n",
    "#                                      remove_columns=[c for c in columns_to_remove if c in train_dataset.column_names])\n",
    "# test_ds_deberta = test_dataset.map(lambda e: preprocess_fast(e, de_tokenizer), batched=True,\n",
    "#                                     remove_columns=[c for c in columns_to_remove if c in test_dataset.column_names])\n",
    "# train_ds_deberta = train_ds_deberta.rename_column('label','labels')\n",
    "# test_ds_deberta  = test_ds_deberta.rename_column('label','labels')\n",
    "# cols_de = ['input_ids','attention_mask','labels']  # DeBERTa does not need token_type_ids for pairs\n",
    "# train_ds_deberta.set_format(type='torch', columns=cols_de)\n",
    "# test_ds_deberta.set_format(type='torch', columns=cols_de)\n",
    "# train_loader_de = DataLoader(train_ds_deberta, batch_size=MICRO_BATCH, shuffle=True, num_workers=0)\n",
    "# test_loader_de  = DataLoader(test_ds_deberta,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=0)\n",
    "# deberta_trainer, deberta_model, deberta_tokenizer = fine_tune_model_directml(\n",
    "#     'microsoft/deberta-base', train_loader_de, test_loader_de, class_weights,\n",
    "#     uses_token_type_ids=False, tokenizer=de_tokenizer\n",
    "# )\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Training evidence: Print sample logs (same UX)\n",
    "# ---------------------------------------------\n",
    "print(\"BERT Training Logs (first 5):\", bert_trainer.state.log_history[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d7a5e-5180-4bb0-b634-db4ad8a932bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ec139-905f-4c58-a21a-e16dde77fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
